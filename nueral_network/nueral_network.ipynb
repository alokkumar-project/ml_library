{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4957a31a-342a-41bf-8d88-6ef8c1dbcbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:95: SyntaxWarning: invalid escape sequence '\\_'\n",
      "<>:95: SyntaxWarning: invalid escape sequence '\\_'\n",
      "/var/folders/qx/m587h9hn4155cbn53_ttrdhc0000gn/T/ipykernel_2629/3735593953.py:95: SyntaxWarning: invalid escape sequence '\\_'\n",
      "  (\\___/)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "class Nueral_Network:\n",
    "    def __init__(self,dataset,y_test,nueron=[3,1],opti='momentum',weight_init='He',type='regression',learning_rate=0.01,epoch=1000):\n",
    "        self.rate = learning_rate\n",
    "        self.count=0\n",
    "        self.nueron=nueron\n",
    "        self.class_index = {}\n",
    "        self.der_z={}\n",
    "        self.der_b={}\n",
    "        self.der_w={}\n",
    "        self.velocity={}\n",
    "        self.m_velocity={}\n",
    "        self.bais_velocity={}\n",
    "        self.m_bais = {}\n",
    "        self.v_bais={}\n",
    "        \n",
    "        if(opti=='momentum'):\n",
    "            self.update_parameter = self.momentum_update\n",
    "        elif(opti=='Adam'):\n",
    "            self.update_parameter=self.adam_update\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        if(nueron[-1]>1):\n",
    "            \n",
    "            self.target=self.one_hot_coding(y_test.values)\n",
    "            \n",
    "        else:\n",
    "            self.target = y_test.values.reshape(-1,1)\n",
    "            \n",
    "        self.layer=len(nueron)\n",
    "        self.node = nueron\n",
    "        \n",
    "        self.bais_velocity['V1']=np.zeros((1,self.node[0]))\n",
    "        self.velocity['V1']=np.zeros((self.node[0],self.dataset.shape[1]))\n",
    "        self.m_velocity['m1']=np.zeros((self.node[0],self.dataset.shape[1]))\n",
    "\n",
    "        self.m_bais['mb1']=np.zeros((1,self.node[0]))\n",
    "        self.v_bais['vb1']=np.zeros((1,self.node[0]))\n",
    "\n",
    "\n",
    "        \n",
    "        for i in range(2,self.layer+1):\n",
    "            self.velocity['V'+str(i)]=np.zeros((self.node[i-1],self.node[i-2]))\n",
    "            self.m_velocity['m'+str(i)]=np.zeros((self.node[i-1],self.node[i-2]))\n",
    "            \n",
    "            self.bais_velocity['V'+str(i)]=np.zeros((1,self.node[i-1]))\n",
    "            self.m_bais['mb'+str(i)]=np.zeros((1,self.node[i-1]))\n",
    "            self.v_bais['vb'+str(i)]=np.zeros((1,self.node[i-1]))\n",
    "            \n",
    "            \n",
    "\n",
    "        \n",
    "        if(nueron[-1]>1):\n",
    "            self.activation=self.softmax\n",
    "            self.loss=self.softmax_loss\n",
    "            self.last_der = self.softmax_last_der\n",
    "            \n",
    "        elif(nueron[-1]==1 and type=='regression'):\n",
    "            self.activation=self.regression\n",
    "            self.loss=self.regression_loss\n",
    "            self.last_der = self.regression_last_der\n",
    "\n",
    "        elif(nueron[-1]==1 and type=='binary'):\n",
    "            self.activation=self.sigmoid\n",
    "            self.loss=self.sigmoid_loss\n",
    "            self.last_der = self.sigmoid_last_der\n",
    "            \n",
    "        if(weight_init=='Xavier'):\n",
    "            self.weights,self.bais = self.Xavier()\n",
    "            \n",
    "        elif(weight_init=='He'):\n",
    "            self.weights,self.bais = self.He()\n",
    "            \n",
    "        for i in range(epoch):\n",
    "            self.z,self.a = self.forward()\n",
    "            y_hat = self.a['A'+str(self.layer)]\n",
    "            loss_cal = self.loss(y_hat)\n",
    "            #===========================backpropagation==============\n",
    "            self.der_z['dz'+str(self.layer)]=self.last_der(y_hat)\n",
    "            self.der_w['dw'+str(self.layer)]=(1/self.dataset.shape[0])*(np.dot((self.der_z['dz'+str(self.layer)]).T,(self.a['A'+str(self.layer-1)])))\n",
    "            self.der_b['db'+str(self.layer)] = (1/self.dataset.shape[0])*np.sum(self.der_z['dz'+str(self.layer)],axis=0,keepdims=True)\n",
    "           #======================last_layer_ko_chhod_kar================\n",
    "            for k in reversed(range(1,self.layer)):\n",
    "                self.der_z['dz'+str(k)]=(np.dot((self.der_z['dz'+str(k+1)]),(self.weights['W'+str(k+1)])))*self.der_relu(self.z['Z'+str(k)])\n",
    "                self.der_w['dw'+str(k)]=(1/self.dataset.shape[0])*np.dot(self.der_z['dz'+str(k)].T,self.a['A'+str(k-1)])\n",
    "                self.der_b['db'+str(k)] = (1/self.dataset.shape[0])*np.sum(self.der_z['dz'+str(k)],axis=0,keepdims=True)\n",
    "            #======================Updating_parameter==================\n",
    "            self.update_parameter()\n",
    "            if(i%100==0):\n",
    "                print(f'Loss after {i} iteration is {loss_cal}')\n",
    "\n",
    "        print(f\"Final loss is {loss_cal}\")\n",
    "        print('''\n",
    "        (\\___/)\n",
    "        (^ _ ^)\n",
    "        (>Training completeðŸ’•\n",
    "        ''')\n",
    "        \n",
    "        \n",
    "\n",
    "    def one_hot_coding(self,y_test):\n",
    "        y_test = np.array(y_test)\n",
    "        \n",
    "        pos = 0\n",
    "        for i in y_test:\n",
    "            if i not in self.class_index:\n",
    "                self.class_index[i]=pos\n",
    "                pos = pos+1\n",
    "        target = []\n",
    "        for i in y_test:\n",
    "            new = []\n",
    "            for j in range(self.nueron[-1]):\n",
    "                if(j==self.class_index[i]):\n",
    "                    new.append(1)\n",
    "                else:\n",
    "                    new.append(0)\n",
    "            target.append(new)\n",
    "        return np.array(target)\n",
    "        \n",
    "    def adam_update(self):\n",
    "        beta1=0.9\n",
    "        beta2=0.99\n",
    "        eps=1e-8\n",
    "        self.count = self.count+1\n",
    "        \n",
    "        for i in range(1,self.layer+1):\n",
    "            self.m_velocity['m'+str(i)]=beta1*self.m_velocity['m'+str(i)]+(1-beta1)*self.der_w['dw'+str(i)]\n",
    "            self.velocity['V'+str(i)]=beta2*self.velocity['V'+str(i)]+(1-beta2)*((self.der_w['dw'+str(i)])**2)\n",
    "            self.weights['W'+str(i)]=self.weights['W'+str(i)]-((self.rate)/((self.velocity['V'+str(i)]+eps)**0.5))*self.m_velocity['m'+str(i)]\n",
    "\n",
    "            self.m_bais['mb'+str(i)]= beta1*self.m_bais['mb'+str(i)]+(1-beta1)*(self.der_b['db'+str(i)])\n",
    "            self.v_bais['vb'+str(i)]= beta2*self.v_bais['vb'+str(i)]+(1-beta2)*((self.der_b['db'+str(i)])**2)\n",
    "\n",
    "            mb_hat = self.m_bais['mb'+str(i)] / (1 - beta1 ** self.count)\n",
    "            vb_hat = self.v_bais['vb'+str(i)] / (1 - beta2 ** self.count)\n",
    "            self.bais['b'+str(i)] = self.bais['b'+str(i)]-(self.rate * mb_hat / (np.sqrt(vb_hat) + eps))\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "    def momentum_update(self):\n",
    "        for i in range(1,self.layer+1):\n",
    "            \n",
    "            self.velocity['V'+str(i)]=0.9*self.velocity['V'+str(i)]+(self.rate)*self.der_w['dw'+str(i)]\n",
    "            self.weights['W'+str(i)] = self.weights['W'+str(i)]-self.velocity['V'+str(i)]\n",
    "\n",
    "            self.bais_velocity['V'+str(i)]=0.9*self.bais_velocity['V'+str(i)]+(self.rate)*self.der_b['db'+str(i)]\n",
    "            self.bais['b'+str(i)] = self.bais['b'+str(i)]-self.bais_velocity['V'+str(i)]\n",
    "            \n",
    "        \n",
    "    def predict(self,test_dataset):\n",
    "        test_dataset = np.array(test_dataset)\n",
    "        if (test_dataset.ndim==1):\n",
    "            test_dataset = test_dataset.reshape(1,-1)\n",
    "        input = test_dataset\n",
    "        for i in range(1,self.layer):\n",
    "            z=np.dot(input,self.weights['W'+str(i)].T)+self.bais['b'+str(i)]\n",
    "            a = self.relu(z)\n",
    "            input =a\n",
    "        z = np.dot(input,self.weights['W'+str(self.layer)].T)+self.bais['b'+str(self.layer)]\n",
    "        output = self.activation(z)\n",
    "        output = np.array(output)\n",
    "        \n",
    "        if(self.activation==self.sigmoid):\n",
    "            #print(\"I am inside activation\")\n",
    "            result =[]\n",
    "            \n",
    "            for i in range(len(output)):\n",
    "                if(output[i]>0.5):\n",
    "                    result.append(1)\n",
    "                else:\n",
    "                    result.append(0)\n",
    "            return result\n",
    "            \n",
    "        if(self.activation==self.softmax):\n",
    "            \n",
    "            result= np.argmax(output,axis=1)\n",
    "            final_result=[]\n",
    "            reverse_dic = {}\n",
    "            \n",
    "            for key,value in self.class_index.items():\n",
    "                reverse_dic[value]=key\n",
    "            \n",
    "            for i in result:\n",
    "                final_result.append(reverse_dic[i])\n",
    "                \n",
    "            return final_result\n",
    "            \n",
    "        return output\n",
    "        \n",
    "            \n",
    "    def forward(self): \n",
    "        z={}\n",
    "        a={}\n",
    "        a['A0']=self.dataset\n",
    "        for i in range(1,self.layer):\n",
    "            z['Z'+str(i)]=np.dot(a['A'+str(i-1)],self.weights['W'+str(i)].T)+self.bais['b'+str(i)]\n",
    "            a['A'+str(i)]=self.relu(z['Z'+str(i)])\n",
    "        z['Z'+str(self.layer)]=np.dot(a['A'+str(self.layer-1)],self.weights['W'+str(self.layer)].T)+self.bais['b'+str(self.layer)]\n",
    "        a['A'+str(self.layer)]=self.activation(z['Z'+str(self.layer)])\n",
    "        return z,a\n",
    "\n",
    "    def sigmoid(self,value):\n",
    "        value = np.clip(value,-500,500)\n",
    "        return 1/(1+np.exp(-value))\n",
    "\n",
    "    def sigmoid_loss(self,y_hat):\n",
    "        eps = 1e-8\n",
    "        loss = -(self.target*(np.log(y_hat+eps))+(1-self.target)*(np.log(1-y_hat+eps)))\n",
    "        return np.mean(loss)\n",
    "    \n",
    "    def sigmoid_last_der(self,y_hat):\n",
    "        return y_hat-self.target\n",
    "        \n",
    "    def regression(self,value):\n",
    "        return value\n",
    "        \n",
    "    def regression_loss(self,y_hat):\n",
    "        #y_hat = np.clip(y_hat, -1e6, 1e6)\n",
    "        loss = (y_hat-self.target)**2\n",
    "        return np.mean(loss)\n",
    "        \n",
    "    def regression_last_der(self,y_hat):\n",
    "        \n",
    "        last=-2*(self.target-y_hat)\n",
    "        return last\n",
    "        \n",
    "\n",
    "        \n",
    "    def softmax(self,z):\n",
    "        z = z - np.max(z, axis=1, keepdims=True)\n",
    "        exp_z = np.exp(z)\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "        \n",
    "        \n",
    "    def softmax_loss(self,y_hat):\n",
    "        eps = 1e-8\n",
    "        y_hat = np.clip(y_hat, eps, 1 - eps)\n",
    "        loss = -(np.sum(self.target*(np.log(y_hat)),axis=1))\n",
    "        return np.mean(loss)\n",
    "        \n",
    "    def softmax_last_der(self,y_hat):\n",
    "        return y_hat-self.target\n",
    "        \n",
    "    def relu(self,z):\n",
    "        return np.maximum(0,z)\n",
    "        \n",
    "    def der_relu(self,z):\n",
    "        return (z > 0).astype(float)\n",
    "        \n",
    "    def Xavier(self):\n",
    "        weights = {}\n",
    "        bais = {}\n",
    "        weights['W1']=np.random.randn(self.node[0],self.dataset.shape[1])*((6/(self.node[0]+self.dataset.shape[1]))**0.5)\n",
    "        bais['b1']=np.zeros((1,self.node[0]))\n",
    "        for i in range(2,self.layer+1):\n",
    "            weights['W'+str(i)]=np.random.randn(self.node[i-1],self.node[i-2])*((6/(self.node[i-1]+self.node[i-2]))**0.5)\n",
    "            bais['b'+str(i)]=np.zeros((1,self.node[i-1]))\n",
    "        return weights,bais\n",
    "    def He(self):\n",
    "        weights = {}\n",
    "        bais = {}\n",
    "        weights['W1']=np.random.randn(self.node[0],self.dataset.shape[1])*((2/(self.dataset.shape[1]))**0.5)\n",
    "        bais['b1']=np.zeros((1,self.node[0]))\n",
    "        for i in range(2,self.layer+1):\n",
    "            weights['W'+str(i)]=np.random.randn(self.node[i-1],self.node[i-2])*((2/(self.node[i-2]))**0.5)\n",
    "            bais['b'+str(i)]=np.zeros((1,self.node[i-1]))\n",
    "        return weights,bais\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d32292fa-aa9c-4db8-89ca-5e624416d579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_f1(y_pre,y_test):\n",
    "    f1_scores =[]\n",
    "    unique_value=np.unique(y_test)\n",
    "    for value in unique_value:\n",
    "        true_pos = np.sum((y_pre==value)&(y_test==value))\n",
    "        false_pos = np.sum((y_pre==value)&(y_test!=value))\n",
    "        false_neg = np.sum((y_pre!=value)&(y_test==value))\n",
    "        true_neg =np.sum((y_pre!=value)&(y_test!=value))\n",
    "        recall = true_pos/(true_pos+false_neg)\n",
    "        precision = true_pos/(true_pos+false_pos)\n",
    "        if(recall+precision==0):\n",
    "            f1_scores.append(0)\n",
    "            continue\n",
    "        f1 = 2*(recall*precision)/(recall+precision)\n",
    "        f1_scores.append(f1)\n",
    "    return np.mean(f1_scores)\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "956d5d6b-b009-4982-a243-0e4891907fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    def __init__(self,neighbour=2):\n",
    "        self.neighbour = neighbour\n",
    "\n",
    "    def cell_dector(self,row):\n",
    "        count = 0\n",
    "        for cell in self.missing_index:\n",
    "            if(cell[0]==row):\n",
    "                count = count+1\n",
    "        return count\n",
    "        \n",
    "    def fill(self,dataset):\n",
    "        self.dataset = dataset.copy()\n",
    "        self.missing_index = np.argwhere(pd.isna(self.dataset))\n",
    "        #print(self.missing_index)\n",
    "        self.col = dataset.shape[1]\n",
    "        distances = []\n",
    "        for missing_cell in self.missing_index:\n",
    "            unfilled_cell = self.cell_dector(missing_cell[0])\n",
    "            filled_cell = self.col-unfilled_cell\n",
    "            distance = []\n",
    "            for i in range(self.dataset.shape[0]):\n",
    "                if(i==missing_cell[0]):\n",
    "                    continue\n",
    "                row_a = self.dataset.iloc[missing_cell[0]]\n",
    "                row_b = self.dataset.iloc[i]\n",
    "                \n",
    "                filled_cell_check = ~(row_a.isna() | row_b.isna())\n",
    "                filled_cell_check.iloc[missing_cell[1]]=False\n",
    "                perfect_a_row = row_a[filled_cell_check]\n",
    "                perfect_b_row = row_b[filled_cell_check]\n",
    "\n",
    "                if len(perfect_a_row) == 0:\n",
    "                    continue\n",
    "                \n",
    "                dis_cal = np.sqrt((filled_cell/self.col)*np.sum((perfect_a_row-perfect_b_row)**2))\n",
    "                distance.append((i,dis_cal))\n",
    "            minimum_index = self.index_finder(distance)\n",
    "            \n",
    "            sum = 0\n",
    "            check = 0\n",
    "            for alok in minimum_index:\n",
    "                if(pd.isna(self.dataset.iloc[alok,missing_cell[1]])):\n",
    "                    continue\n",
    "                    \n",
    "                sum = sum+self.dataset.iloc[alok,missing_cell[1]]\n",
    "                check = check+1\n",
    "            if (check>0):\n",
    "                fill_values = sum/check\n",
    "                self.dataset.iloc[missing_cell[0],missing_cell[1]]=fill_values\n",
    "            \n",
    "        return self.dataset \n",
    "    def index_finder(self, distance):\n",
    "        index = []\n",
    "        value = []\n",
    "        for dist in distance:\n",
    "            index.append(dist[0])\n",
    "            value.append(dist[1])\n",
    "        value = np.array(value)\n",
    "        index = np.array(index)\n",
    "        minimum_value = value.argsort()\n",
    "        minimum_index = index[minimum_value][:self.neighbour]\n",
    "        return minimum_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "286d52fd-f18e-4f72-b855-c5437a0ff67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scaler:\n",
    "    def __init__(self):\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "        \n",
    "    def scale(self,dataset):\n",
    "        \n",
    "        self.mean = dataset.mean()\n",
    "        self.std = dataset.std()\n",
    "        \n",
    "    \n",
    "    def transform(self,dataset):\n",
    "        new_dataset = (dataset-self.mean)/(self.std)\n",
    "        return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4b663df-e88d-49f2-9dcb-010d27b3c6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_test_split:\n",
    "    def __init__(self,dataset,test_size=0.2,random=1,target='target'):\n",
    "        self.target = target\n",
    "        self.random_state=random\n",
    "        self.test_size=test_size\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        self.row = self.dataset.shape[0]\n",
    "\n",
    "\n",
    "    def split(self):\n",
    "        \n",
    "        np.random.seed(self.random_state)\n",
    "        row_count = int(self.row*self.test_size)\n",
    "        value = np.arange(0,self.row)\n",
    "        row_no = np.random.choice(value,replace=False,size=row_count)\n",
    "        \n",
    "\n",
    "        test_dataset = self.dataset.iloc[row_no]\n",
    "        x_test = test_dataset.drop(columns=[self.target])\n",
    "        y_test = test_dataset[self.target]\n",
    "\n",
    "        remaining_row=[]\n",
    "        for i in range(self.row):\n",
    "            if i not in row_no:\n",
    "                remaining_row.append(i)\n",
    "        train_dataset = self.dataset.iloc[remaining_row]\n",
    "        x_train = train_dataset.drop(columns=[self.target])\n",
    "        y_train = train_dataset[self.target]\n",
    "        \n",
    "        return x_train,x_test,y_train,y_test\n",
    "        \n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271f84e0-fea3-4e53-a8cd-d96c23501de8",
   "metadata": {},
   "source": [
    "# **For Multiclass classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9796c803-168b-4b26-be5a-2e52db799521",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train_nn.csv')\n",
    "\n",
    "data = data.iloc[:,1:]\n",
    "\n",
    "target = data['target_cls']\n",
    "data.drop(columns=['target_reg','target_cls'],inplace=True)\n",
    "s=Scaler()\n",
    "s.scale(data)\n",
    "data = s.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91aa00f3-2cba-4ff8-abd7-06e2389162fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 0 iteration is 4.746548088365848\n",
      "Loss after 100 iteration is 0.24547402914398658\n",
      "Loss after 200 iteration is 0.09978452561327279\n",
      "Loss after 300 iteration is 0.043613263111143635\n",
      "Loss after 400 iteration is 0.016821918126348513\n",
      "Loss after 500 iteration is 0.006267465602925876\n",
      "Loss after 600 iteration is 0.002515174642306827\n",
      "Loss after 700 iteration is 0.0011499577682820097\n",
      "Loss after 800 iteration is 0.0005951071862565018\n",
      "Loss after 900 iteration is 0.000350424177948912\n",
      "Final loss is 0.00023176795254145928\n",
      "\n",
      "        (\\___/)\n",
      "        (^ _ ^)\n",
      "        (>Training completeðŸ’•\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "n = Nueral_Network(data,target,weight_init='He',nueron=[64,32,26],opti='Adam')\n",
    "test_data = pd.read_csv('test_nn.csv')\n",
    "test_data=test_data.iloc[:,1:]\n",
    "test_data = s.transform(test_data)\n",
    "predicted_class=n.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "59d90a3a-69fc-4624-a8d9-d419cad658f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train_nn.csv')\n",
    "\n",
    "data = data.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a3aab47e-01fe-4336-8c1d-5f88bf8158e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['target_cls'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d05f971-e0d5-4b94-97a0-c663d4de8db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "id = np.arange(test_data.shape[0])\n",
    "nn_submit=pd.DataFrame({\n",
    "    'id':id,\n",
    "    'Target_cls':predicted_class\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e59e9dda-66eb-401b-bf22-70341104da32",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_submit.to_csv('nn_multi_submit.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba33438-3461-450b-8975-eda9074a24ab",
   "metadata": {},
   "source": [
    "# **Regression Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3e88697-5190-4762-97d2-adbf6d2e2385",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train_nn.csv')\n",
    "\n",
    "data = data.iloc[:,1:]\n",
    "\n",
    "target = data['target_reg']\n",
    "data.drop(columns=['target_reg','target_cls'],inplace=True)\n",
    "s=Scaler()\n",
    "s.scale(data)\n",
    "data = s.transform(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d9b1f5c-519e-4833-adcf-b04488d5d820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 0 iteration is 61.98043340825482\n",
      "Loss after 100 iteration is 1.6377373314529364\n",
      "Loss after 200 iteration is 0.15805772630925446\n",
      "Loss after 300 iteration is 0.038372038461542406\n",
      "Loss after 400 iteration is 0.09069461360403135\n",
      "Loss after 500 iteration is 0.017023283055053195\n",
      "Loss after 600 iteration is 0.00797879665820567\n",
      "Loss after 700 iteration is 0.005273388898054108\n",
      "Loss after 800 iteration is 0.006383458728148289\n",
      "Loss after 900 iteration is 0.008168478217888273\n",
      "Loss after 1000 iteration is 0.017829394941353313\n",
      "Loss after 1100 iteration is 0.006577669801974116\n",
      "Loss after 1200 iteration is 0.00467839731994173\n",
      "Loss after 1300 iteration is 0.01381093148046481\n",
      "Loss after 1400 iteration is 0.0053810488433984295\n",
      "Loss after 1500 iteration is 0.0013608500912771651\n",
      "Loss after 1600 iteration is 0.0010372377536383155\n",
      "Loss after 1700 iteration is 0.0012727468341500253\n",
      "Loss after 1800 iteration is 0.0077217256551946846\n",
      "Loss after 1900 iteration is 0.016150508765656192\n",
      "Loss after 2000 iteration is 0.000601050651473207\n",
      "Loss after 2100 iteration is 0.0012445311797330647\n",
      "Loss after 2200 iteration is 0.0013702564687738384\n",
      "Loss after 2300 iteration is 0.001802020962599615\n",
      "Loss after 2400 iteration is 0.001497212121211513\n",
      "Loss after 2500 iteration is 0.004967794231073183\n",
      "Loss after 2600 iteration is 0.008384637921681837\n",
      "Loss after 2700 iteration is 0.003920903674708356\n",
      "Loss after 2800 iteration is 0.0019187263141553607\n",
      "Loss after 2900 iteration is 0.004242061262718067\n",
      "Loss after 3000 iteration is 0.0005675228550908185\n",
      "Loss after 3100 iteration is 0.004296183687197413\n",
      "Loss after 3200 iteration is 0.0005910005702710123\n",
      "Loss after 3300 iteration is 0.0039035271211440565\n",
      "Final loss is 0.008369269824271391\n",
      "\n",
      "        (\\___/)\n",
      "        (^ _ ^)\n",
      "        (>Training completeðŸ’•\n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[7.3375999 ],\n",
       "       [7.77183411],\n",
       "       [6.2437877 ],\n",
       "       [8.70726675]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = Nueral_Network(data,target,weight_init='He',type='regression',nueron=[512,256,128,64,1],opti='Adam',epoch=3400)\n",
    "test_data = pd.read_csv('test_nn.csv')\n",
    "test_data=test_data.iloc[:,1:]\n",
    "test_data = s.transform(test_data)\n",
    "n.predict(data.iloc[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fac1652a-5fb6-45c6-a971-5995de5d18a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_y = n.predict(test_data)\n",
    "pre_y = pre_y.ravel()\n",
    "ids = np.arange(test_data.shape[0])\n",
    "\n",
    "nn_submit = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'Target_reg': pre_y\n",
    "})\n",
    "\n",
    "nn_submit.to_csv('nn_regression_submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb21d64-076c-476a-a45e-a0df124aa31a",
   "metadata": {},
   "source": [
    "## **For Multi-Class classification-2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e091f1eb-730a-4e01-88cc-275b7a48a89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = pd.read_csv('train_multi_class_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3de68c93-b8e6-494e-b839-4526e1a19c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data3.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3e2d16bf-ba72-4fc3-91cd-2cb65b9b7cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Train_test_split(data3)\n",
    "x_train,x_test,y_train,y_test = t.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a9a601a9-b6b0-480c-9235-178be174f5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Scaler()\n",
    "s.scale(x_train)\n",
    "x_train=s.transform(x_train)\n",
    "x_test=s.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "03e8f184-bcbf-454b-b3a4-1fe9e2efd98f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_class = np.unique(data3['target']).shape[0]\n",
    "unique_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c5f98052-7970-4d7e-b53c-1934b30d7cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 0 iteration is 2.3528074061507036\n",
      "Loss after 100 iteration is 0.8747077614900789\n",
      "Loss after 200 iteration is 0.6886818967241356\n",
      "Loss after 300 iteration is 0.631408280472146\n",
      "Loss after 400 iteration is 0.6008635704139007\n",
      "Loss after 500 iteration is 0.5806935659340768\n",
      "Loss after 600 iteration is 0.5680965332845108\n",
      "Loss after 700 iteration is 0.5539760410303776\n",
      "Loss after 800 iteration is 0.5511649244465784\n",
      "Loss after 900 iteration is 0.5409608500093638\n",
      "Loss after 1000 iteration is 0.5361869782097468\n",
      "Loss after 1100 iteration is 0.5330363449844404\n",
      "Loss after 1200 iteration is 0.5374068222349391\n",
      "Loss after 1300 iteration is 0.5290647196735095\n",
      "Loss after 1400 iteration is 0.5261312349644047\n",
      "Loss after 1500 iteration is 0.5220712586280108\n",
      "Loss after 1600 iteration is 0.5189644125236214\n",
      "Loss after 1700 iteration is 0.5170582710965527\n",
      "Loss after 1800 iteration is 0.5141840188425805\n",
      "Loss after 1900 iteration is 0.5116177034915318\n",
      "Loss after 2000 iteration is 0.5097962037777694\n",
      "Loss after 2100 iteration is 0.5090094612726055\n",
      "Loss after 2200 iteration is 0.507159998230239\n",
      "Loss after 2300 iteration is 0.5056265321830493\n",
      "Loss after 2400 iteration is 0.5045847539661013\n",
      "Loss after 2500 iteration is 0.5028999695999662\n",
      "Loss after 2600 iteration is 0.5036163278827831\n",
      "Loss after 2700 iteration is 0.5031558923380569\n",
      "Loss after 2800 iteration is 0.499415946327937\n",
      "Loss after 2900 iteration is 0.49921096380240393\n",
      "Final loss is 0.4992177609280774\n",
      "\n",
      "        (\\___/)\n",
      "        (^ _ ^)\n",
      "        (>Training completeðŸ’•\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "n = Nueral_Network(x_train,y_train,nueron=[32,16,8,unique_class],learning_rate=0.1,epoch=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8e459510-09df-4228-bdcf-c679e5d873ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_value = n.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "89873882-1f09-49a6-8319-8a43f00b83df",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = macro_f1(pre_value,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "078472fc-00dd-4efd-8deb-41d3d0c9402d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8153533169505064)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ebc806-89fe-41e7-aa89-23aea3f929cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
